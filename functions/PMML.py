import os
from enum import Enum
from pathlib import Path
import pandas as pd
import pypmml
import sklearn.metrics as sklearn_metrics
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.tree import DecisionTreeClassifier
import xml.etree.ElementTree as ElementTreeXML

from helpers.logger import print_and_log


class ModelName(Enum):
    NAIVE_BAYES = "naive bayes"
    DECISION_TREE = "decision tree"
    LINEAR_REGRESSION = "linear regression"
    K_MEANS = "k-means"
    MLP = "multi-layer perceptron (Neural Network)"


class PMMLModel:
    """
    PMML model class to make predictions, save the predictions and metrics to a CSV file and train and
    validate the model

    Attributes:
    ----------
    inputDatasetFilePath: str
        Input dataset filepath
    outputDatasetFilePath: str
        Output dataset filepath
    pmml_model_filepath: str
        PMML model filepath
    inputDataset: pd.DataFrame
        Input dataset
    predictions: pd.DataFrame
        Predictions made by the model
    validation_predictions: pd.DataFrame
        Predictions to validate the model
    pmmlModelLearner: pypmml.Model
        PMML model learner
    exportOnlyPredictions: bool
        Export just the predictions or the whole dataset with predictions
    export_test_metrics: bool
        Export the test metrics or not
    algorithmName: str
        Algorithm type name
    modelName: ModelName
        Algorithm model name
    inputNames: list
        Input column names used by the model
    outputNames: list
        Output column names generated by the model
    predictedClassColumn: str
        Predicted class column name
    classColumn: str
        Class column name to predict
    metric_scores: pd.DataFrame
        Metric scores
    train_split: float
        Train split percentage for the dataset
    test_split: float
        Test split percentage for the dataset
    """
    inputDatasetFilePath: str = None  # Input dataset filepath
    outputDatasetFilePath: str = None  # Output dataset filepath
    pmml_model_filepath: str = None  # PMML model filepath

    inputDataset: pd.DataFrame = None  # Input dataset
    predictions: pd.DataFrame = None  # Predictions made by the model
    validation_predictions: pd.DataFrame = None  # Validation predictions

    pmmlModelLearner: pypmml.Model = None  # PMML model learner
    exportOnlyPredictions: bool = None  # Export just the predictions or the whole dataset with predictions
    export_test_metrics: bool = None  # Export the test metrics or not

    algorithmName: str = None  # Algorithm type name
    modelName: ModelName = None  # Algorithm model name

    inputNames: list = None  # Input column names used by the model
    outputNames: list = None  # Output column names generated by the model

    predictedClassColumn: str = None  # Predicted class column name
    classColumn: str = None  # Predicted class column name

    metric_scores: pd.DataFrame = None  # Metric scores

    train_split: float = None  # Train split percentage for the dataset
    test_split: float = None  # Test split percentage for the dataset

    def __init__(self, input_dataset_filepath: str, output_dataset_filepath: str, model_learner_pmml_filepath: str,
                 export_only_predictions: bool, export_test_metrics: bool, train_split: float = None,
                 test_split: float = None):
        """
        Initialize the PMML model object, parse the PMML model and make and export the predictions

        :param input_dataset_filepath: filepath to the input dataset to use to validate the model and make predictions
        :param output_dataset_filepath: filepath to the output dataset to save the predictions and metrics
        :param model_learner_pmml_filepath: filepath to the PMML model file
        :param export_only_predictions: export just the predictions or the whole dataset with predictions
        :param export_test_metrics: boolean to export the test metrics or not
        :param train_split: train split percentage for the dataset
        :param test_split: test split percentage for the dataset
        """
        self.inputDatasetFilePath = input_dataset_filepath
        self.outputDatasetFilePath = output_dataset_filepath
        self.pmml_model_filepath = model_learner_pmml_filepath

        self.inputDataset = pd.read_csv(self.inputDatasetFilePath)  # Load the input dataset as a dataframe
        self.pmmlModelLearner = pypmml.Model.load(model_learner_pmml_filepath)  # Load the PMML model
        self.exportOnlyPredictions = export_only_predictions  # Export just the predictions or the whole
        # dataset with predictions
        self.export_test_metrics = export_test_metrics  # Export the test metrics or not
        # Save the train and test split if they are not None
        if train_split is not None and test_split is not None:
            self.train_split = train_split
            self.test_split = test_split

        # MAIN FUNCTION CALLS: PARSE PMML MODEL, MAKE AND EXPORT PREDICTIONS AND SAVE TEST METRICS
        self.parser_pmml_model()  # Parse the PMML model
        self.make_and_export_predictions()  # Make and export the predictions
        self.save_test_metrics()  # Save the test metrics
        print_and_log(f"{self.modelName.value} model has been loaded\n")

    def get_model_name(self):
        """
        Get a uniform model name from the PMML model name
        """
        if self.pmmlModelLearner.modelName is not None:
            pmml_model_name = self.pmmlModelLearner.modelName
        else:
            pmml_model_name = self.pmmlModelLearner.modelElement
        model_name_lower = pmml_model_name.lower()
        best_match = None
        highest_score = 0

        # Calculate the score based on substring matches with variable lengths
        for name in ModelName:
            name_value = name.value.lower()
            score = 0

            # Iterate through all possible substrings of name_value
            for i in range(len(name_value)):
                for j in range(i + 1, len(name_value) + 1):
                    substring = name_value[i:j]
                    if substring in model_name_lower:
                        score += 1

            # Save the best match
            if score > highest_score:
                highest_score = score
                best_match = name

        self.modelName = best_match

    def parser_pmml_model(self):
        """
        Parse the PMML model
        """
        self.algorithmName = self.pmmlModelLearner.functionName  # Get the function name
        self.get_model_name()
        self.inputNames = self.pmmlModelLearner.inputNames
        self.outputNames = self.pmmlModelLearner.outputNames

        # Get the predicted class column name
        if self.algorithmName != 'clustering':
            self.predictedClassColumn = self.outputNames[0]  # Get the predicted class column name
            self.classColumn = self.predictedClassColumn.replace('predicted_', '')  # Get class column name

    def make_and_export_predictions(self):
        """
        Make and export the predictions
        """
        print_and_log("Making predictions on the dataset...")
        self.predictions = self.pmmlModelLearner.predict(self.inputDataset)  # Make predictions on the dataset

        # Create the output directory if it does not exist
        if not os.path.exists(self.outputDatasetFilePath):
            os.makedirs(self.outputDatasetFilePath)

        if not self.exportOnlyPredictions:
            predictions_df = self.inputDataset.copy()
            predictions_df = pd.concat([predictions_df, self.predictions], axis=1)
            predictions_df.to_csv(f'{self.outputDatasetFilePath}/{Path(self.inputDatasetFilePath).stem}'
                                  f'_with_predictions_using_{self.modelName.name}.csv', index=False)
            print_and_log("Predictions and dataset saved to a CSV file")
        else:
            self.predictions.to_csv(f'{self.outputDatasetFilePath}/{Path(self.inputDatasetFilePath).stem}'
                                    f'_only_predictions_using_{self.modelName.name}.csv', index=False)
            print_and_log("Only predictions saved to a CSV file")

    def save_test_metrics(self):
        """
        Save the test metrics to the PMMLModel class instance and export them to a CSV file if the attribute
        'export_test_metrics' is True
        """
        print_and_log("Testing model and saving metric results...")
        if self.algorithmName == 'classification':

            y_true = self.inputDataset[self.classColumn]
            y_pred = self.predictions[self.predictedClassColumn]
            y_true = pd.Categorical(y_true).codes
            y_pred = pd.Categorical(y_pred).codes

            accuracy = sklearn_metrics.accuracy_score(y_true, y_pred)
            precision = sklearn_metrics.precision_score(y_true, y_pred, average='macro')
            recall = sklearn_metrics.recall_score(y_true, y_pred, average='macro')
            f1 = sklearn_metrics.f1_score(y_true, y_pred, average='macro')
            # Save the metrics to the PMMLModel object
            self.metric_scores = pd.DataFrame({
                'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],
                'Score': [accuracy, precision, recall, f1]
            })

        elif self.algorithmName == 'regression':

            y_true = self.inputDataset[self.classColumn]
            y_pred = self.predictions[self.predictedClassColumn]
            mse = sklearn_metrics.mean_squared_error(y_true, y_pred)
            r2 = sklearn_metrics.r2_score(y_true, y_pred)
            mae = sklearn_metrics.mean_absolute_error(y_true, y_pred)
            median_ae = sklearn_metrics.median_absolute_error(y_true, y_pred)

            self.metric_scores = pd.DataFrame({
                'Metric': ['Mean Squared Error', 'R2 Score', 'Mean Absolute Error', 'Median Absolute Error'],
                'Score': [mse, r2, mae, median_ae]
            })

        elif self.algorithmName == 'clustering':
            cluster_names = self.predictions['cluster_name']

            # Count the number of instances in each cluster
            cluster_counts = cluster_names.value_counts()

            self.metric_scores = pd.DataFrame({
                'Metric': ['Cluster Counts'],
                'Score': [cluster_counts]
            })

        else:
            raise ValueError(f"{self.modelName.value} model not recognized")

        if self.export_test_metrics:
            if not os.path.exists(self.outputDatasetFilePath):
                os.makedirs(self.outputDatasetFilePath)

            self.metric_scores.to_csv(f'{self.outputDatasetFilePath}/{Path(self.inputDatasetFilePath).stem}'
                                      f'_metric_scores_using_{self.modelName.name}.csv', index=False)
            print_and_log("Metric scores saved to a CSV file")

    def train_and_validate_model(self):
        """
        Train and validate the model if the attribute 'validate_model' is True
        """
        print_and_log(f"Training and validating the {self.modelName.value} model...")

        model_validated = False

        # Split the dataset into train and test using train_test_split from sklearn
        x = self.inputDataset[self.inputNames]
        y = self.inputDataset[self.classColumn] if self.classColumn is not None else None
        y_test = None
        y_train = None
        if self.classColumn is not None:
            x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=self.train_split,
                                                                test_size=self.test_split, shuffle=True)
        else:
            x_train, x_test = train_test_split(x, train_size=self.train_split, test_size=self.test_split, shuffle=True)

        # Prepare the training data to convert categorical columns to numerical columns
        x_train = x_train.apply(lambda col: pd.Categorical(col).codes)
        x_test = x_test.apply(lambda col: pd.Categorical(col).codes)

        validation_model = None
        # Train the model using the training dataset
        if self.modelName == ModelName.K_MEANS:
            # Carga el archivo PMML
            tree = ElementTreeXML.parse(self.pmml_model_filepath)
            root = tree.getroot()

            # Encuentra el ClusteringModel y accede a 'numberOfClusters'
            clustering_model = root.find('.//{http://www.dmg.org/PMML-4_2}'+ self.pmmlModelLearner.modelElement)
            if clustering_model is not None:
                number_of_clusters = int(clustering_model.get('numberOfClusters'))
                validation_model = KMeans(n_clusters=number_of_clusters)
                validation_model.fit(x_train)
            else:
                print_and_log("Clustering model not found")

        elif self.modelName == ModelName.LINEAR_REGRESSION:
            validation_model = LinearRegression()
            validation_model.fit(x_train, y_train)
        elif self.modelName == ModelName.DECISION_TREE:
            validation_model = DecisionTreeClassifier()
            validation_model.fit(x_train, y_train)
        elif self.modelName == ModelName.NAIVE_BAYES:
            validation_model = GaussianNB()
            validation_model.fit(x_train, y_train)
        elif self.modelName == ModelName.MLP and self.algorithmName == 'classification':
            validation_model = MLPClassifier()
            validation_model.fit(x_train, y_train)
        elif self.modelName == ModelName.MLP and self.algorithmName == 'regression':
            validation_model = MLPRegressor()
            validation_model.fit(x_train, y_train)
        else:
            raise ValueError(f"{self.modelName.value} model not recognized")

        print_and_log(f"{self.modelName.value} model has been TRAINED")

        # Make validation predictions
        self.validation_predictions = validation_model.predict(x_test)

        # Validate the model
        if self.algorithmName == 'classification':

            y_true = y_test
            y_pred = self.validation_predictions
            y_true = pd.Categorical(y_true).codes
            y_pred = pd.Categorical(y_pred).codes

            validated_accuracy = sklearn_metrics.accuracy_score(y_true, y_pred)
            validated_precision = sklearn_metrics.precision_score(y_true, y_pred, average='macro')
            validated_recall = sklearn_metrics.recall_score(y_true, y_pred, average='macro')
            validated_f1 = sklearn_metrics.f1_score(y_true, y_pred, average='macro')

            pretrained_model_accuracy = self.metric_scores.loc[self.metric_scores['Metric'] == 'Accuracy', 'Score'].values[0]
            pretrained_model_precision = self.metric_scores.loc[self.metric_scores['Metric'] == 'Precision', 'Score'].values[0]
            pretrained_model_recall = self.metric_scores.loc[self.metric_scores['Metric'] == 'Recall', 'Score'].values[0]
            pretrained_model_f1 = self.metric_scores.loc[self.metric_scores['Metric'] == 'F1 Score', 'Score'].values[0]

            print_and_log(f"Pretrained Model Accuracy: {pretrained_model_accuracy} - Validation Model Accuracy: {validated_accuracy}")
            print_and_log(f"Pretrained Model Precision: {pretrained_model_precision} - Validation Model Precision: {validated_precision}")
            print_and_log(f"Pretrained Model Recall: {pretrained_model_recall} - Validation Model Recall: {validated_recall}")
            print_and_log(f"Pretrained Model F1 Score: {pretrained_model_f1} - Validation Model F1 Score: {validated_f1}")

            # Check if the metrics are the same considering an epsilon error
            if ((pretrained_model_accuracy - validated_accuracy) < 1e-5 and
                    (pretrained_model_precision - validated_precision) < 1e-5 and
                    (pretrained_model_recall - validated_recall) < 1e-5 and
                    (pretrained_model_f1 - validated_f1) < 1e-5):
                print_and_log("Metrics are the same")
                model_validated = True

        elif self.algorithmName == 'regression':

            y_true = y_test
            y_pred = self.validation_predictions
            validated_mse = sklearn_metrics.mean_squared_error(y_true, y_pred)
            validated_r2 = sklearn_metrics.r2_score(y_true, y_pred)
            validated_mae = sklearn_metrics.mean_absolute_error(y_true, y_pred)
            validated_median_ae = sklearn_metrics.median_absolute_error(y_true, y_pred)

            pretrained_model_mse = self.metric_scores.loc[self.metric_scores['Metric'] == 'Mean Squared Error', 'Score'].values[0]
            pretrained_model_r2 = self.metric_scores.loc[self.metric_scores['Metric'] == 'R2 Score', 'Score'].values[0]
            pretrained_model_mae = self.metric_scores.loc[self.metric_scores['Metric'] == 'Mean Absolute Error', 'Score'].values[0]
            pretrained_model_median_ae = self.metric_scores.loc[self.metric_scores['Metric'] == 'Median Absolute Error', 'Score'].values[0]

            print_and_log(f"Pretrained Model Mean Squared Error: {pretrained_model_mse} - Validation Model Mean Squared Error: {validated_mse}")
            print_and_log(f"Pretrained Model R2 Score: {pretrained_model_r2} - Validation Model R2 Score: {validated_r2}")
            print_and_log(f"Pretrained Model Mean Absolute Error: {pretrained_model_mae} - Validation Model Mean Absolute Error: {validated_mae}")
            print_and_log(f"Pretrained Model Median Absolute Error: {pretrained_model_median_ae} - Validation Model Median Absolute Error: {validated_median_ae}")

            # Check if the metrics are the same considering an epsilon error
            if ((pretrained_model_mse - validated_mse) < 1e-5 and (pretrained_model_r2 - validated_r2) < 1e-5 and
                    (pretrained_model_mae - validated_mae) < 1e-5 and
                    (pretrained_model_median_ae - validated_median_ae) < 1e-5):
                print_and_log("Metrics are the same")
                model_validated = True

        elif self.algorithmName == 'clustering':

            # Count the number of instances in each cluster and compare with the original cluster counts
            validated_cluster_counts = pd.Series(validation_model.labels_).value_counts()
            pretrained_model_cluster_counts = self.metric_scores.loc[self.metric_scores['Metric'] == 'Cluster Counts', 'Score'].values[0]
            print_and_log(f"Pretrained Model Cluster Counts: {pretrained_model_cluster_counts} - Validation Model Cluster Counts: {validated_cluster_counts}")

            # Check if the cluster counts are the same considering an epsilon error
            if (pretrained_model_cluster_counts - validated_cluster_counts).abs().max() < 1e-5:
                print_and_log("Cluster counts are the same")
                model_validated = True
            else:
                print_and_log("Cluster counts are NOT the same")
        else:
            raise ValueError(f"{self.modelName.value} model not recognized")

        if model_validated:
            print(f"{self.modelName.value} model has been VALIDATED\n")
            print_and_log(f"{self.modelName.value} model has been validated\n")
        else:
            print(f"{self.modelName.value} model has NOT BEEN VALIDATED\n")
            print_and_log(f"{self.modelName.value} model has NOT BEEN validated\n")

    def __str__(self):
        """
        String representation of the PMML model
        :return: string representation of the PMML model
        """
        return f"Algorithm type name: {self.algorithmName}\n" \
               f"Model name: {self.modelName.value}\n" \
               f"Input names: {self.inputNames}\n" \
               f"Output names: {self.outputNames}\n"
